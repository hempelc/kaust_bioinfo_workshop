{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94c2acfa-a7cf-49cb-b8bd-458138c9da71",
   "metadata": {},
   "source": [
    "# KAUST bioinformatics workshop\n",
    "### By Chris Hempel (christopher.hempel@kaust.edu.sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4ea7c-b8ae-4284-a0f4-205d2a39fe7e",
   "metadata": {},
   "source": [
    "Follow along by copying the code into your IBEX terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f47d54-8ea4-4253-a436-6fac601f16dc",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86320b1-269b-45ed-bef4-ec4483ce79eb",
   "metadata": {},
   "source": [
    "### 1.1 Working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2e0e1-0718-43fb-b662-a694936d171c",
   "metadata": {},
   "source": [
    "Make sure you're in your home directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed4002-e89a-4a56-8eed-324823be18ba",
   "metadata": {},
   "source": [
    "`cd ~`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f89530-a2b4-4f66-bc1b-e9632d304154",
   "metadata": {},
   "source": [
    "Then download the GitHub directory for this workshop to set up a working directory. All code will be run and programs will be installed in this directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e60b8-8059-4e19-a4c4-9545deb93a14",
   "metadata": {},
   "source": [
    "`git clone https://github.com/hempelc/kaust_bioinfo_workshop.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d950f-5dca-4320-bf57-e1a369744020",
   "metadata": {},
   "source": [
    "### 1.2 Program installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a0c36-38b5-4d7b-982a-1cdd85e61955",
   "metadata": {},
   "source": [
    "Generate and navigate to a directory in the working directory called \"programs\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ceac13-65b1-45d4-b013-9cf96c15b491",
   "metadata": {},
   "source": [
    "`mkdir ~/kaust_bioinfo_workshop/programs`\n",
    "\n",
    "`cd ~/kaust_bioinfo_workshop/programs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d4ab4-6306-4fcf-9023-bf4d97e5fb46",
   "metadata": {},
   "source": [
    "#### 1.2.1 Installing mamba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc392b-8924-491e-bb1e-7bb160afb05c",
   "metadata": {},
   "source": [
    "Mamba allows you to generate environments in which you can install packages (=programs). That way, you don't mix up the requirements for different programs you want to run. We install mamba following the instructions [here](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26804c65-80e0-4c4a-9f88-66191d75225b",
   "metadata": {},
   "source": [
    "`wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"`\n",
    "\n",
    "`bash Miniforge3-$(uname)-$(uname -m).sh`\n",
    "\n",
    "`rm Miniforge3-$(uname)-$(uname -m).sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5b155-d6dc-4c76-ac85-56884ab4eeca",
   "metadata": {},
   "source": [
    "#### 1.2.2 Installing FastQC and MultiQC using mamba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b4736-6259-4f37-b272-2a2696ea4df3",
   "metadata": {},
   "source": [
    "You can find instructions on how to install certain packages using the [anaconda search engine](https://anaconda.org/).\n",
    "For example, multiqc can be installed as shown [here](https://anaconda.org/bioconda/multiqc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42789624-3d5f-4f26-a20f-6390b8953298",
   "metadata": {},
   "source": [
    "We create a new mamba environment using `mamba create` and give it a name with the `-n` parameter. Then we state what packages we want to install. In this case, we want to install fastqc and multiqc (to determine the quality of our sequences, more on that later), which are both found on the package installation channel bioconda:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c4dd6-163c-40b3-96ef-d84197a0af86",
   "metadata": {},
   "source": [
    "`mamba create -n multiqc bioconda::fastqc bioconda::multiqc`\n",
    "\n",
    "Alternatively, we can use the parameter `-c` to state that mamba should check the channel bioconda for the installation:\n",
    "\n",
    "`mamba create -n multiqc -c bioconda fastqc multiqc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e5b9e-4d5b-4c59-9bed-e36d4b1b981c",
   "metadata": {},
   "source": [
    "#### 1.2.3 Installing Apscale using a combination of mamba and GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6af672-bac8-4e4a-b4fe-d2066bff528e",
   "metadata": {},
   "source": [
    "[Apscale](https://github.com/DominikBuchner/apscale) is a very fast and easy-to-use metabarcoding processing pipeline. We will later use it to process some sequencing data. We will install a modified version of Apscale that I generated, as well as a wrapper for Apscale to easily run it in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832db84-1c93-4579-803f-5eb151108f7a",
   "metadata": {},
   "source": [
    "First, we will set up a mamba environment in which we will install all required packages/programs for Apscale. We could type all programs into one command to install them in the environment, but we need to install a lot of them with specific versions, which is a lot to type. Therefore, we will use a .yml file for the installation instead. The .yml file contains all information on packages to be installed, including versions, and can be read by mamba. This ensures that all of us install exactly the same version of the Apscale mamba environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ec7de-74de-427f-ba54-0dd463bacead",
   "metadata": {},
   "source": [
    "Clone the GitHub directory containing my modified version of Apscale to retrieve the required .yml file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14752892-63c5-4809-9761-c4acd9a5a048",
   "metadata": {},
   "source": [
    "`git clone https://github.com/hempelc/apscale.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6a2a8-3e46-4fc2-bf69-50330c4478b8",
   "metadata": {},
   "source": [
    "Create the mamba env directly from the .yml file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de92bba-c3ff-4ceb-ad88-13bda943676f",
   "metadata": {},
   "source": [
    "`mamba env create -f apscale/conda-env_apscale_IBEX.yml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1f60ba-3da5-4564-b78d-59545c4b3f55",
   "metadata": {},
   "source": [
    "Now, all required packages for Apscale are installed, just Apscale itself is missing. To install Apscale, we activate the environment and install the modified version of Apscale with the setup.py file located in the cloned GitHub directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10f435-746b-4766-839a-8e164596cc4a",
   "metadata": {},
   "source": [
    "`mamba activate apscale`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b98f8-ff09-412e-a47f-8b1b71ee18b3",
   "metadata": {},
   "source": [
    "`cd apscale`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e76a8-5ba2-47f1-8da3-a6102668e8b5",
   "metadata": {},
   "source": [
    "`python setup.py install`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be0ab1-38d3-4a0a-9025-826d438c89dd",
   "metadata": {},
   "source": [
    "`cd ~/kaust_bioinfo_workshop/programs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d838f31-6074-47cb-94c0-1eaf15a74694",
   "metadata": {},
   "source": [
    "Deactivate the environment again. I'd recommend activating mamba environments only before you need them, just to make sure you don't accidentally install something into your environment and mess it up:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf609d-7b0d-4cb3-9629-d388ce5f1970",
   "metadata": {},
   "source": [
    "`mamba deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cf389-a1b4-48c8-9859-5d187e22e941",
   "metadata": {},
   "source": [
    "Lastly, we need to install the Apscale wrapper. Therefore, clone another of my GitHub directories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2019b15f-c7fd-49e2-8e74-54960e1044bf",
   "metadata": {},
   "source": [
    "`git clone https://github.com/hempelc/apscale_wrapper.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082e198-28f2-4755-8f6e-79266b5eabd3",
   "metadata": {},
   "source": [
    "And make the wrapper scripts available in your PATH (otherwise they won't work):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42ae4e-3a9f-47d8-a29b-76ffcf4d2a8e",
   "metadata": {},
   "source": [
    "`echo -e '\\n#Manually exported paths\\nexport PATH=$PATH:/home/'\"$(whoami)\"'/programs/apscale_wrapper' >> ~/.bash_profile && source ~/.bash_profile` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec768e4-c3fb-4e00-9b8b-3e914c526e8b",
   "metadata": {},
   "source": [
    "#### 1.2.4 Installing Krona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0bc71-1027-4640-a902-968c1eb51fd9",
   "metadata": {},
   "source": [
    "[Krona](https://github.com/marbl/Krona/wiki) allows you to generate great interactive visualizations for metabarcoding data. The visualizations can be used for publications, but their main use it to interactively explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739b69e-d12d-4aba-a501-5301f0cc22be",
   "metadata": {},
   "source": [
    "Create a new mamba environment for krona:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4cf58-a1aa-4442-93fd-c6056e65d047",
   "metadata": {},
   "source": [
    "`mamba create -n krona bioconda::krona`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4b9d0-f1e3-44a1-ba04-a04db0fef2c3",
   "metadata": {},
   "source": [
    "Activate the environment and run a script that came with krona to install a krona taxonomy database:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad7a9f-5079-4812-b1ac-c90b02bd4ca3",
   "metadata": {},
   "source": [
    "`mamba activate krona && ktUpdateTaxonomy.sh && mamba deactivate krona`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a03c00-1006-4cca-a41e-6c9fbee5ba6a",
   "metadata": {},
   "source": [
    "#### 1.2.5 Installing Cyberduck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5c5cb-e3be-4f7c-88f0-2dbdeb99bcb0",
   "metadata": {},
   "source": [
    "On your local computer (not on IBEX!): download and install [Cyberduck](https://cyberduck.io/download/). Cyberduck allows you to connect to IBEX and view and download your files with a graphic user interface, just like you're used to!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56d091-cb89-408d-8af9-4610ab47bf89",
   "metadata": {},
   "source": [
    "## 2. Download sequencing data (proudly sponsored by Ken) and the MIDORI2 COI BLAST database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c29e05-4015-48d2-89ce-fba2e822bb7b",
   "metadata": {},
   "source": [
    "Move back the working directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df716021-ef35-4e2e-bfa2-80e90620bd8f",
   "metadata": {},
   "source": [
    "`cd ~/kaust_bioinfo_workshop`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192abba-b6f1-4077-aa9a-551563d05ff6",
   "metadata": {},
   "source": [
    "All data we need is accessible on my Google Drive. You can download it using the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdec6f-fe30-469f-9b77-0dfcf4868667",
   "metadata": {},
   "source": [
    "`wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Cookie: CONSENT=PENDING+238; SOCS=CAESHAgCEhJnd3NfMjAyNDAxMDItMF9SQzQaAmVuIAEaBgiAmfKsBg; SID=fAgtNWQlH3PTQE33XKx_z5pNwzPj-pde5PGFKH1_FIlUiK_-uzzZnxsd_rE5mCEp83FqVg.; __Secure-1PSID=fAgtNWQlH3PTQE33XKx_z5pNwzPj-pde5PGFKH1_FIlUiK_-l8agA_b1J4uQawJ8DG1P3w.; __Secure-3PSID=fAgtNWQlH3PTQE33XKx_z5pNwzPj-pde5PGFKH1_FIlUiK_--iPUnxAdhQEYBdcMVOfVWg.; HSID=AY895b6yQh3ERBnKf; SSID=AYo1w_ypkxdWNJZp6; APISID=zW20S9QhUHd5O6BQ/A3ahD13pc-c7fiu5h; SAPISID=vInNTptJkQe-o4ZR/A-gXcuimBrHpwkjBL; __Secure-1PAPISID=vInNTptJkQe-o4ZR/A-gXcuimBrHpwkjBL; __Secure-3PAPISID=vInNTptJkQe-o4ZR/A-gXcuimBrHpwkjBL; AEC=Ae3NU9Pcnwo7fX1Y44YCSH1DD97iIkOTQ1hF5mcn6RcSB4GoNQsYkezF1d8; NID=511=N5eBkHK_g_3jKNGBOSAbaWKR8uU3gfv7-Yj6zFcKYxwq8yifhRWkGillvUx76Ym9qlaoosfzj0M8vDDE9chSGcMpFkyLSJ0kTeskiihIjjbxXpR1XEs1DNuHHb4EngByMLdOH3T2_CbOJwjc4UkDNx5j8ezCJbJ-Xo4Vj80bXegGkJeW-mMeh0RPHJej_iaDllBGfgBZmK65rytVId-6ZmzZw3fjtUDX9w6ROJet4udX7Y_v8cfMzpr8_VGtWMNEwF8UAEENQKLEIRsSFO6qsCMbPkwkGN3ozjwAjhxDFtfOP60hQlBDB07o44gwlZt88w_jr1YS0eZrytbaLhyRaSMiv3JtB7o4Tfy_iD3cMWhUX9Tah6CTIZnKLDIbZQwM4ofosoAho-WsQ8JFOvDqSkTayID5MyXJsNax7PmsJiHzIkM3F7vun-B1Bxv_EW5Hfqg; 1P_JAR=2024-01-19-23; __Secure-ENID=17.SE=phTR0YJslGNoHRdC7V5qpmCYazlGyDzZNTty0q6xooH5qAdxe8wNArCqPTs8xnX_kEjamFr5q6rKakaFNPzLN8PYUyaCT6T4-1QFMJ0RHM0b-0dAmCgxC9oiFzW7nnOrxVQvyNU7mamZKxiimTraF1Wri6U9beW0J8uTsnVlOVUv0q2KGgi67dYqjD7hh9AoLt0aaZDuyr9aV2tYQqq8Fj6kOV3Aej-Tfj5gImFIwhnrihjTWLE5dmMqHxIapt0BlKG4wOtHh4Sb; __Secure-1PSIDTS=sidts-CjIBPVxjSvc6nIvT0_2MEjKb0q6VKdKIxwX8n9OtTaQXMSj3VwmSHOUaxVsPUf-Wnwho2RAA; __Secure-3PSIDTS=sidts-CjIBPVxjSvc6nIvT0_2MEjKb0q6VKdKIxwX8n9OtTaQXMSj3VwmSHOUaxVsPUf-Wnwho2RAA; SIDCC=ABTWhQGtYTjREbjYswNUUslB4UhhMB1mprMRB1EqqPb_3Fm0OnRQQZPVpuBLRuoSmOutXtP0yIiX; __Secure-1PSIDCC=ABTWhQHTTNCTiznY3ld2rbu1LtLi6rpBLOZNbmrlLbydOfZnE0QGG0Hv_i1f-1VTiBTKa9jq4zk; __Secure-3PSIDCC=ABTWhQGUR5aPEShdmYCFnHgbBIbhNEAY_ks4vJmpFMri5hkoQMxvfZXog_6ZfhH1X-RCKzQA5ls\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1134kQbJC76VyXe8Kdy-MfE_XSG1KIA7f&export=download&authuser=0&confirm=t&uuid=73de8872-8f7e-4950-9e4f-d97222fc0be4&at=APZUnTWBelFkm3Ab6e2cHF3AflDn:1705708397053\" -c -O 'kaust_bioinfo_workshop_files.tar.gz'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae94be-7225-44fd-b083-a4e01c721e8a",
   "metadata": {},
   "source": [
    "Lastly, uncompress it so that we can access the files:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45561a8-445e-4aa8-a0b1-fd2068b9cec5",
   "metadata": {},
   "source": [
    "`tar -xzvf kaust_bioinfo_workshop_files.tar.gz`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b20288-b98d-4336-ab83-c607a0056a39",
   "metadata": {},
   "source": [
    "`rm kaust_bioinfo_workshop_files.tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b88b6e-9e1b-4085-a558-f73aed6e29cd",
   "metadata": {},
   "source": [
    "## 3. Analyze the quality of the sequencing data using MultiQC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f49636-6da1-4f78-b6a2-9db31a47fb5e",
   "metadata": {},
   "source": [
    "Let's have a look at the quality of Ken's sequencing data (I just included a few samples). We will use the programs FastQC and MultiQC to get an overview of the quality. We installed both programs in a mamba environment, so we just need to activate the environment to be able to use the programs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5b7c3-cf30-4ae1-9035-02fab9d55f43",
   "metadata": {},
   "source": [
    "`mamba activate multiqc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059054f-6a23-475e-b70b-7794a1f20c5e",
   "metadata": {},
   "source": [
    "First, generate a directory for the output of FastQC. Then, run FastQC on the samples and save the output in the generated output directory. Note: we don't run this command with SBATCH because it's a very small, simple job:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77ed82-9456-47b7-909d-c6464c4b5978",
   "metadata": {},
   "source": [
    "`mkdir fastqc_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f0bdc-65e5-40f4-93a9-bceedaa6bb39",
   "metadata": {},
   "source": [
    "`fastqc kaust_bioinfo_workshop_files/example_sequences/* -o fastqc_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a11845-cd0c-4459-8956-6c558a70d9c1",
   "metadata": {},
   "source": [
    "Then run MultiQC on the FastQC output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed459b77-fb63-46a7-b6c8-0ebdb7701e1c",
   "metadata": {},
   "source": [
    "`multiqc --interactive -o multiqc_output fastqc_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca1b77-d70e-45f8-8bbb-10c8ffb4676e",
   "metadata": {},
   "source": [
    "The quality summary is saved under `multiqc_output/multiqc_report.html`. It's an HTML file so we will have to download it to our local computer and open it in a web browser. Open Cyberduck, download the MultiQC HTML file to your local computer, and let's check out the file together. But before we do that, don't forget to deactivate your mamba environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1b03c-9e43-4d89-a8fc-a5602edf628e",
   "metadata": {},
   "source": [
    "`mamba deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c578008-1c7e-488c-a616-567085025bd0",
   "metadata": {},
   "source": [
    "## 4. Process the sequencing data with Apscale "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d135f6f-9119-4a4e-99d3-ca0527a03415",
   "metadata": {},
   "source": [
    "When we ran FastQC and MultiQC above, we just ran them in the login terminal of IBEX because they're small jobs. However, running Apscale is not a small job, as it requires GBs of RAM and multiple cores (the more cores, the faster it runs). Therefore, to run Apscale, we make use of IBEX' computational power by submitting a script to the job scheduler (SLURM). In the script, we list our commands to run Apscale. Then, SLURM will send our code to one of the many powerful computers within the IBEX computer cluster and run the code there. The results of the run are sent back to our user account, specifically to the location where we started the run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a07c293-933b-42d6-8967-b315b0e928c1",
   "metadata": {},
   "source": [
    "To submit a job via SLURM, we need to configure our script in a specific format. You will find a pre-formatted script to run Apscale in the working directory. Open it with the following command and let's have a look at it together:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf39e8-1991-4c0e-a183-d421c49ce5ed",
   "metadata": {},
   "source": [
    "`cat kaust_bioinfo_workshop_apscale_sbatch.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be959f-9be7-4056-bad8-9e6463324b2a",
   "metadata": {},
   "source": [
    "You will notice a bunch of parameters for our Apscale run. To find more information on these parameters, you can open the help page of the Apscale wrapper with the following command (note: we activate the mamba Apscale environment to run the help command and deactivate it after):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303ca44-06f1-4e9d-b5b7-dac664004fd4",
   "metadata": {},
   "source": [
    "`mamba activate apscale && apscale_wrapper.py --help && mamba deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee51869-d683-4a84-8279-cb71861d2e26",
   "metadata": {},
   "source": [
    "Now, let's submit the SLURM script to the job scheduler using the command sbatch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f523ea6-1d02-48fd-8baf-88eb54bb23a3",
   "metadata": {},
   "source": [
    "`sbatch kaust_bioinfo_workshop_apscale_sbatch.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93616cf-6028-45d4-8a32-8ab40f4ae570",
   "metadata": {},
   "source": [
    "You can find additional information on your run via the sacct command, which displays all your submitted runs as well as their used RAM, CPU time, runtime, and more (note: I modified the command to separate runs by lines):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c7d79-7863-4545-b85f-5ed78596d837",
   "metadata": {},
   "source": [
    "`term_width=$(tput cols) && line=\"\" && for ((i=0; i<term_width; i++)); do line+=\"-\"; done && sacct --units G --format=\"JobID,JobName%30,start%10,ncpus%5,MaxRSS%7,time,Elapsed,mincpu,state\" --starttime 2023-05-01 | sed \"s/extern.*$/&\\n${line}/\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534842d-9c57-4f94-a289-ca527aec276a",
   "metadata": {},
   "source": [
    "You can also check what your run is currently doing by viewing the log file that is generated for the run. The log file contains all text output that is generated by your script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448045f-63f2-42dc-a33b-15ee704aad76",
   "metadata": {},
   "source": [
    "`cat kaust_bioinformatics_workshop_apscale.*.log`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320d0ed-c6e4-49ee-8308-530428d46aaa",
   "metadata": {},
   "source": [
    "Once the run is done, let's investigate the files generated by Apscale by downloading them to our local computer using Cyberduck. Simultaneously, download [this](https://github.com/hempelc/kaust_bioinfo_workshop/blob/main/metabarcoding_overview.pptx) PowerPoint presentation to learn about the different metabarcoding processing steps while we're investigating Apscale's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3e4cd-6dc6-4e7e-83b3-a684edb31f4b",
   "metadata": {},
   "source": [
    "## 5. EXTRA: Setting up a BLAST database from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124fbf00-0c9f-4ac0-80fe-383059b6e515",
   "metadata": {},
   "source": [
    "We used a curated BLAST database to annotate our OTU sequences, specifically [MIDORI2](https://www.reference-midori.info/), which curates sequences from NCBI for many metabarcoding target genes. Generally, when processing metabarcoding data, such curated databases are the way to go, as they are more reliable and targeted than assembling a database from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816f5a9-5fe7-48f4-8610-fe28d17abc4e",
   "metadata": {},
   "source": [
    "However, there are cases where it is beneficial or necessary to generate a BLAST database yourself. Let's go through the required steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bba72a-71ed-4759-8b1e-65a71d58753a",
   "metadata": {},
   "source": [
    "First, you need a FASTA file containing the sequences you want to add to your database. A good starting point is going to [NCBI GenBank](https://www.ncbi.nlm.nih.gov/genbank/) and searching for sequences for your target organism(s) and gene(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e3f15-d8cc-46bc-8a14-1d359a955db9",
   "metadata": {},
   "source": [
    "In Ken's data, most of the sequences are assigned to an OTU belonging to the family \"Myctophidae\". Since his data is a gut-content sample, this OTU should stem from his fish host. Let's see how good of a match we get when BLASTing this OTU against all Phosichthyidae COI sequences on NCBI GenBank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65eb20d-be98-4442-ac28-fdb97e822bc8",
   "metadata": {},
   "source": [
    "In the search field of [NCBI GenBank](https://www.ncbi.nlm.nih.gov/genbank/), search for <u>**\"Myctophidae\" AND \"COI\"**</u>. At the point of writing this tutorial, there are 265 hits=sequences. Let's download all of them in one FASTA file and upload them to IBEX with Cyberduck. I already did this for you, so you will find the FASTA file located in the workshop files, but if you wanted to do this yourself, you would click on \"Send to\", \"Choose destination=File\", and \"Format=FASTA\", and start the download by clicking \"Create file\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443bc871-ee4f-48b6-992f-7c36b04e3fe3",
   "metadata": {},
   "source": [
    "Check out the content of the directory containing the prepared FASTA files. You will also find a FASTA file containing Ken's 1st OTU:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af1ef5a-9d64-4948-9085-2b66928dd26c",
   "metadata": {},
   "source": [
    "`ls kaust_bioinfo_workshop_files/blast_db_setup`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29401fb-57f3-48cf-b5b3-1a41cc6341a5",
   "metadata": {},
   "source": [
    "Print and inspect the prepared BLAST sbatch script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d9db6-eb1f-47ac-aca1-24d2991023b0",
   "metadata": {},
   "source": [
    "`cat kaust_bioinfo_workshop_blast_sbatch.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6aa4e5-66b0-4071-9268-a549dce191b9",
   "metadata": {},
   "source": [
    "Submit the job:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89496d73-02f5-469f-a858-fff5b9da9417",
   "metadata": {},
   "source": [
    "`sbatch kaust_bioinfo_workshop_blast_sbatch.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf848c8-388d-443e-bd5d-16ce4407c7b2",
   "metadata": {},
   "source": [
    "And let's investigate the output together:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc74c79-f607-4777-86e2-2476d6929b71",
   "metadata": {},
   "source": [
    "`cat blast_output.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c7d10-183f-4573-b446-de5c4357b043",
   "metadata": {},
   "source": [
    "Notice anything? Open the other blast output generated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c1bef-6f22-4da2-991e-94dbf0c621c7",
   "metadata": {},
   "source": [
    "`cat blast_modified_output.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837994f-4022-4ab6-b08a-0d439e5157dc",
   "metadata": {},
   "source": [
    "**<u>Important!</u>** All of this can be done via the [BLAST website](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome) as well - for a single sequence, i.e., one OTU. However, the advantage of the command line-based approach is that you can blast as many sequences as you want at once against the database, so potentially 1,000s of OTUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69af63b-5cd0-4662-b6b8-aa758e459312",
   "metadata": {},
   "source": [
    "## 6. EXTRA: Krona graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2822a-1dfe-49b0-9ef5-05252ac3395b",
   "metadata": {},
   "source": [
    "Let's turn the taxonomic data of Ken's samples into a Krona graph. We need to format the data to do so. I generated a script to generate the right format. The script is written in bash to ensure that everyone can run it on IBEX, but bash is not the best coding language for such tasks. If you need to format a file on your own, you can also use R, python, or even Excel to achieve the desired format, which might be more intuitive. For now, run the script like so (we specify the path to the file we want to format and the sample ID prefix to tell the code which columns in the file are samples, in our case that is \"LF\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01140f7d-2156-422f-9009-c6af022480db",
   "metadata": {},
   "source": [
    "`./krona_formatting.sh kaust_bioinformatics_workshop_apscale/9_lulu_filtering/otu_clustering/kaust_bioinformatics_workshop_apscale_OTU_table_filtered_microdecon-filtered_with_taxonomy.csv LF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0dc00-55d8-4b4f-9b85-b4a60e93cbb6",
   "metadata": {},
   "source": [
    "The generated, formatted file is called \"krona_formatted_file.tsv\". Let's have a look at the first few lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8d8ce-44a5-4b2f-abf4-e948df4a49a9",
   "metadata": {},
   "source": [
    "`head krona_formatted_file.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f213809-9806-4af8-b4f9-491665594848",
   "metadata": {},
   "source": [
    "Now, activate the krona mamba environment and run the krona script that turns a text file into a krona graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c89bfd-25a4-4abf-8698-8265e6ec582a",
   "metadata": {},
   "source": [
    "`mamba activate krona && ktImportText -o krona.html krona_formatted_file.tsv && mamba deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e4031-0f5e-4dc5-b4fa-6db43c504400",
   "metadata": {},
   "source": [
    "Download the generated file \"krona.html\" with Cyberduck and let's inspect it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7809d-603d-41b2-8233-2531bb351308",
   "metadata": {},
   "source": [
    "## 7. EXTRA: ChatGPT and Copilot for coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a7c4e-e901-46ca-88ec-928938d7a0bf",
   "metadata": {},
   "source": [
    "Type into ChatGPT:\n",
    "\n",
    "\"You're an experienced bioinformatician who is generating an R script for a bioinformatic workshop on metabarcoding data processing. Write an R script that uses phyloseq to explore the structure of an OTU table and generates multiple different nice visualizations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d34d2-3128-4fd3-a0f0-f830819ea2c3",
   "metadata": {},
   "source": [
    "Type:\n",
    "\n",
    "\"In a phyloseq object in R, I want to remove a few OTUs by name (for example, OTU_1). How?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726c42e-0fc5-43c9-95d6-3b2e6e223f6d",
   "metadata": {},
   "source": [
    "## 8. EXTRA: DADA2 on IBEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf779015-fad9-428a-aaca-9ad94ee033ef",
   "metadata": {},
   "source": [
    "Running DADA2 on IBEX is a little bit more tricky than running, for example, Apscale, as you need to install DADA2 in your local R directory and need to setup an R script that you then upload to IBEX and run via SBATCH. First, load the R module, then generate a directory in which installed R packages are saved, and open R:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b1d26-a721-45ce-bbe0-3ecbc1769aa7",
   "metadata": {},
   "source": [
    "```\n",
    "module load R\n",
    "mkdir -p $R_LIBS\n",
    "R\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10efd5-28d4-49c1-8dfb-4b5c7da54341",
   "metadata": {},
   "source": [
    "In R, type the following line by line to install dada2 and some other required packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b574da5-51c9-45c9-b665-4d88ec679e11",
   "metadata": {},
   "source": [
    "```\n",
    "install.packages(\"BiocManager\") # Select mirror \"1\"\n",
    "BiocManager::install(\"dada2\")\n",
    "BiocManager::install(\"phyloseq\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855ab22-f9a8-4701-8ddc-1be13931d7a9",
   "metadata": {},
   "source": [
    "Locate the R file \"dada2_master_script.R\" in the kaust_bioinfo_workshop GitHub directory, open it with R Studio, and adjust the initial variables as required, which means you need to specify the path on IBEX to your sequences etc. Note: the script requires reads that were already primer-trimmed with the tool \"cutadapt\", so you have to run cutadapt before running the dada2 script. This means you need to install cutadapt in a mamba environment and use it to trim the primers of your raw sequences like shown in [this](https://cutadapt.readthedocs.io/en/stable/guide.html) user guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f749cea0-0ef2-4224-8e78-37d6c603fd89",
   "metadata": {},
   "source": [
    "Then, upload the modified dada2_master_script.R script to IBEX. In the same folder, upload the SLURM script \"dada2_sbatch.sh\". Finally, on IBEX, submit the SLURM script via `sbatch dada2_sbatch.sh`. IMPORTANT: The R and SLURM script have to be in the same location and all your variables specified in the R script need to be correct, otherwise it won't work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
